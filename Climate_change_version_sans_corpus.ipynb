{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf64745f",
   "metadata": {},
   "source": [
    "Partie 1 : chargement des donnees \n",
    "1.1 Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53268705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Author import Author\n",
    "from document import document\n",
    "\n",
    "def add_document(id2doc, id2aut, id_counter, titre, auteur, date, url, texte):\n",
    "    \"\"\"\n",
    "    Ajoute un document au corpus et met à jour les auteurs.\n",
    "    \n",
    "    Retourne : nouveau id_counter (incrémenté)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Création du document\n",
    "    doc = document(titre, auteur, date, url, texte)\n",
    "\n",
    "    # 2) Ajout dans id2doc\n",
    "    doc_id = f\"doc_{id_counter}\"\n",
    "    id2doc[doc_id] = doc\n",
    "    id_counter += 1\n",
    "\n",
    "    # 3) Ajout auteur(s)\n",
    "    # Si plusieurs auteurs : séparer par virgules\n",
    "    author_list = [a.strip() for a in auteur.split(\",\")]\n",
    "\n",
    "    for name in author_list:\n",
    "        if name not in id2aut:\n",
    "            id2aut[name] = Author(name)\n",
    "\n",
    "        id2aut[name].add(doc_id, doc)\n",
    "\n",
    "    return id_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf8e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Searching in r/climate \n",
      "\n",
      " Searching in r/climatechange \n",
      "\n",
      " Searching in r/environment \n",
      "\n",
      " Searching in r/sustainability \n",
      "\n",
      " Searching in r/carbonfootprint \n",
      "\n",
      " Searching in r/ecology \n",
      "\n",
      " Searching in r/greenenergy \n",
      "\n",
      " Searching in r/climateaction \n",
      "\n",
      "Nombre de documents Reddit : 26\n",
      "Nombre d'auteurs : 26\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from datetime import datetime\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='_qZmFflED2oGl3LmqZvqBg',\n",
    "    client_secret='1wQi5AamyCOBlq5HQqbpu85PDRjySQ',\n",
    "    user_agent='projet_api'\n",
    ")\n",
    "\n",
    "mot_cles = [\n",
    "    'climate', 'climatechange', 'environment', 'sustainability',\n",
    "    'carbonfootprint', 'ecology', 'greenenergy', 'climateaction'\n",
    "]\n",
    "\n",
    "id2doc = {}\n",
    "id2aut = {}\n",
    "id_counter = 0\n",
    "\n",
    "for mot in mot_cles:\n",
    "    print(f\"\\n Searching in r/{mot} \")\n",
    "    posts = reddit.subreddit(mot).top(limit=20)\n",
    "\n",
    "    for post in posts:\n",
    "        text = post.selftext.replace(\"\\n\", \" \").strip()\n",
    "        if text == \"\":\n",
    "            continue\n",
    "\n",
    "        title = post.title\n",
    "        author_name = str(post.author) if post.author else \"Unknown\"\n",
    "        date = datetime.fromtimestamp(post.created_utc).strftime(\"%Y-%m-%d\")\n",
    "        url = f\"https://reddit.com{post.permalink}\"\n",
    "\n",
    "        #  Ajouter le document\n",
    "        id_counter = add_document(\n",
    "            id2doc, id2aut, id_counter,\n",
    "            titre=title,\n",
    "            auteur=author_name,\n",
    "            date=date,\n",
    "            url=url,\n",
    "            texte=text\n",
    "        )\n",
    "\n",
    "print(\"\\nNombre de documents Reddit :\", id_counter)\n",
    "print(\"Nombre d'auteurs :\", len(id2aut))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6221078",
   "metadata": {},
   "source": [
    "1.2 Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d23c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import xmltodict\n",
    "from document import document\n",
    "from Author import Author\n",
    "from datetime import datetime\n",
    "import certifi\n",
    "import ssl\n",
    "\n",
    "ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "theme = \"climatechange\" \n",
    "print(\"\\n--- Requesting Arxiv ---\")\n",
    "\n",
    "url = f\"https://export.arxiv.org/api/query?search_query=all:{theme}&start=0&max_results=20\"\n",
    "response = urllib.request.urlopen(url, context=ssl_context)\n",
    "xml_data = response.read()\n",
    "\n",
    "# Parsing XML en dictionnaire Python\n",
    "data = xmltodict.parse(xml_data)\n",
    "entries = data[\"feed\"][\"entry\"]\n",
    "\n",
    "# Si un seul document retourné → mettre dans une liste\n",
    "if isinstance(entries, dict):\n",
    "    entries = [entries]\n",
    "\n",
    "for entry in entries:\n",
    "    title = entry.get(\"title\", \"No title\").strip()\n",
    "    abstract = entry.get(\"summary\", \"\").replace(\"\\n\", \" \").strip()\n",
    "    url = entry.get(\"id\", \"\")\n",
    "    date = entry.get(\"published\", \"unknown\")\n",
    "\n",
    "    # Récupération des noms d'auteurs\n",
    "    authors = entry.get(\"author\", [])\n",
    "    if isinstance(authors, dict):\n",
    "        authors = [authors]\n",
    "\n",
    "    author_names_list = [a[\"name\"] for a in authors]\n",
    "    author_names_str = \", \".join(author_names_list)\n",
    "\n",
    "    #  Ajouter le document\n",
    "    id_counter = add_document(\n",
    "        id2doc,\n",
    "        id2aut,\n",
    "        id_counter,\n",
    "        titre=title,\n",
    "        auteur=author_names_str,     # string : \"Alice, Bob\"\n",
    "        date=date,\n",
    "        url=url,\n",
    "        texte=abstract\n",
    "    )\n",
    "\n",
    "print(\"\\nNombre de documents Arxiv ajoutés :\", len(entries))\n",
    "print(\"Taille totale du corpus :\", len(id2doc))\n",
    "print(\"Nombre total d'auteurs :\", len(id2aut))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06f113",
   "metadata": {},
   "source": [
    "Partie 2 : sauvegarde des données\n",
    "2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceccd427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Corpus sauvegardé dans climatechange_corpus_test.csv\n",
      " Nombre total de documents : 26\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "filename = f\"{theme.replace(' ', '_')}_corpus.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    #  Charger directement le CSV\n",
    "    df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    print(f\" Corpus chargé depuis {filename} ({len(df)} documents)\")\n",
    "else:\n",
    "    #  Construire le DataFrame à partir de id2doc\n",
    "    data = []\n",
    "\n",
    "    for doc_id, doc in id2doc.items():\n",
    "        data.append({\n",
    "            \"id\": doc_id,\n",
    "            \"titre\": doc.titre,\n",
    "            \"auteur\": doc.auteur,\n",
    "            \"date\": doc.date,\n",
    "            \"url\": doc.url,\n",
    "            \"texte\": doc.texte\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    #  Sauvegarde avec tabulation (\\t)\n",
    "    df.to_csv(filename, sep=\"\\t\", index=False)\n",
    "    print(f\" Corpus sauvegardé dans {filename}\")\n",
    "    print(f\" Nombre total de documents : {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e55aa3",
   "metadata": {},
   "source": [
    "Partie 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deffc1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taille du corpus\n",
    "df.shape\n",
    "#Afficher le Nombre de mots et de phrase pour chaque doc\n",
    "df[\"word_count\"] = df[\"texte\"].apply(lambda x: len(str(x).split()))\n",
    "df['sentence_count']=df['texte'].apply(lambda x : len(str(x).split('.')))\n",
    "df\n",
    "#Supprimer les documents trop petits (moins de 20c)\n",
    "df_new=df[df['word_count']>=20]\n",
    "df.shape\n",
    "df_new.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae99b90",
   "metadata": {},
   "source": [
    "Statistiques d'un auteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ed960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Auteur : Mountain-Blacksmith7\n",
      "Nombre de documents produits : 1\n",
      "Taille moyenne des documents : 202.00 mots\n"
     ]
    }
   ],
   "source": [
    "name = input(\"\\nEntrez le nom d'un auteur pour obtenir ses statistiques : \")\n",
    "\n",
    "if name in id2aut:\n",
    "    aut = id2aut[name]\n",
    "\n",
    "    # 1) Nombre de documents\n",
    "    print(f\"\\nAuteur : {aut.name}\")\n",
    "    print(f\"Nombre de documents produits : {aut.ndoc}\")\n",
    "\n",
    "    # 2) Taille moyenne des documents\n",
    "    #    On compte le nombre de mots dans chaque texte\n",
    "    total_words = sum(len(doc.texte.split()) for doc in aut.production.values())\n",
    "    moyenne = total_words / aut.ndoc if aut.ndoc > 0 else 0\n",
    "\n",
    "    print(f\"Taille moyenne des documents : {moyenne:.2f} mots\")\n",
    "\n",
    "else:\n",
    "    print(\"Auteur inconnu dans le corpus.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
